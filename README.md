# hand_on_transformer
code to realize transformer
## 原理
[知乎](https://zhuanlan.zhihu.com/p/338817680)
## 实现步骤
1. 把序列转化为矩阵（词嵌入）
2. 由于transformer结构中没有包含词之间的先后顺序，所以需要位置编码；
编码公式采用正弦函数，一是可以适合很多长度，而是利用正弦函数求和公式可以得到不同位置之间的相互关系
3. 多头注意力，利用不同的权重矩阵来实现自注意力，多头就是多个注意力机制，最后拼接得到和X维度相同的中间矩阵
4. resnet连接，层归一化，来增加稳定性
5. 两个线性层进一步提取特征。到此实现Encoder模块
6. Decoder 模块，一是掩码多头注意力，主要是$QK^T$部分的时候乘一个零一矩阵
7. 第二个多头注意力使用Encoder模块输出的得到KV，掩码部分得到查询矩阵
8. 最后加一个线性层，和softmax回归得到不同词的预测概率

## 代码说明
维度处理：在所有关键模块中添加了维度注释，确保张量形状正确

多头注意力：使用标准的缩放点积注意力机制

位置编码：实现为可复用的模块，支持自动广播

模块化设计：将模型组件拆分为独立模块，方便扩展

测试用例：使用简单的复制任务验证模型有效性

扩展性
通过修改Config类参数可快速调整模型大小

编码器结构可直接用于BERT实现

通过添加预训练任务可扩展为GPT等模型

## 代码实现注意
1. 包结构，首先需要确保pycache文件有相关的缓存文件，没有的话需要先运行一下得到缓存文件。